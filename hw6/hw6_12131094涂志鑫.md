## Advanced Control for Robotics - Homework 6

####<p align="right"> 涂志鑫 12131094 </p>
####<p align="right"> 2022.05 </p>  

$\bf1.$ $\textbf{Schur complement lemma}$: Find an equivalent semidefinite condition of the form $G(x) ⪰ 0$ for each of the following statements. Make sure the matrix G(x) you obtained is affine w.r.t. $x$, where $x$ is a vector or matrix variable of appropriate dimension. Please show your steps.
(a) (Singular value bound): $σ(A(x)) < β$, where $A$ : $Rn → R^{q×m}$ is affine in x ∈ Rn and σ(·) denotes the singular value of a matrix.
(b) (Riccati inequality): $A^Tx+xA+xBR^{−1}B^{T}x+Q ≺ 0$, with $x ∈ Sn$ , $R ∈ Sp$ , $Q ∈ Sn$
and $A ∈ R^{n×n}$, and $B ∈ R^{n×p}$.

###### &emsp;Solution:
&emsp;&emsp; 

<br>

$\bf2.$ $ \textbf{Ellipsoid}$ : Ellipsoid in $R^n$ have two equivalent representations: (i) $E_1(P,x_c) = {x ∈ R^n : (x−x_c)P −1(x−x_c) ≤ 1}$ and 
(ii) $E_2(A, x_c) = {Au+x_c : ∥u∥^2 ≤ 1}$. The second representation can be derived from the first by letting $A = P^{\frac{1}{2}}$. Given $E_1(P,xc)$ with $P ∈ {S_{++}^n}$, its volume is ${\nu_n \sqrt {det(P)}}$ where $\nu_n$ is the volume of unit ball in ${R^n}$, its semi-axes directions are given by the eigenvectors of $P$ and the lengths of semi-axes are $\sqrt{λ_i}$, where $\sqrt{λ_i}$ are eigenvalues of $P$.

(a) Given a half space {x ∈ Rn : aT x ≤ 1}. Show that the Ellipsoid E2(A, 0) is contained in the eigenvectors of P and the lengths of semi-axes are the half space if and only if $a^TAA^Ta ≤ 1$.

(b) Note that for any $P ∈ S_{++}^n$, the function $log(det(P))$ is concave in the matrix variable $P$. Formulate a convex optimization problem to find the matrix $P ∈ S_{++}^n$ such that $E_1(P, 0)$ is the largest ellipsoid contained in the polyhedron ${x ∈ R^n : a^T_i x ≤ 1,i = 1,...,m}$

(c) Use Drake to solve the above problem with $a^T_1 = [−1, 1]$, $a^T_2 = [2, −1]$, $a^T_3 = [1, 3]$, $a^T_4 = [−2, −5]$. Visualize the polyhedron region and your ellipsoid solution (you can use Matlab for the visualization if you prefer matlab)

###### &emsp;Solution
&emsp;&emsp; 
<br>

$\bf3.$ Consider a discrete time system $x(k + 1) = Ax(k) + Bu(k)$, with linear feedback law $u(k) = −Kx(k)$. Write down the closed-loop dynamics, and derive conditions for $V (x) = x^T Px$ to be discrete time Lyapunov function for asymptotic closed-loop stability.


###### &emsp;Solution
&emsp;&emsp; The closed-loop discrete time system is $x(k + 1) = (A-BK)x(k)$.

&emsp;&emsp; The initial condition of the system is $x(0)$, when $k = 1$, $x(1) = (A-BK)x(0)$; when $k=2$, $x(2) = (A-BK)^2x(0)$. 

&emsp;&emsp; We can know the dynamics of cloosed-loop system, $x(K) = (A-BK)^kx(0)$.

&emsp;&emsp; For system to be asymptotically stable, the conditions is $(1): V(x)$ is PD. (2) Rate of change of a function $V(x)$ along the system trajectory $\triangle_f V(x) <0$.

&emsp;&emsp; That means for (1): $P$ is PD matrix. 
&emsp;&emsp; For (2): $$\begin{aligned}\triangle_f V(x)  &= V(x(K+1)) - V(x(k)) = V((A-BK)x) - V(x)\\
&= x^T\left[ (A-BK)P(A-BK)-P \right]x<0  \end{aligned}$$
that is $\left[ (A-BK)P(A-BK)-P \right]$ is ND matrix.

$\bf4.$ Show that the PSD cone is acute, i.e., $∀A,B ∈Sn^+$, we have $tr(AB) ≥0$. (Hint: decompose A using unitary matrix Q, i.e. $A = QΛQ^T$ , and then use the same $Q$ to define another matrix $C = QBQ^T$ . The trace $tr(AB)$ can be computed directly in terms of the entries in $C$ and $Λ$)

###### &emsp;Proof
&emsp;&emsp; According to the "Spectral decomposition", for $∀A∈Sn$, there exit a unitary matrix $Q$ and a diagonal matrix $Λ$ satisfy $A = QΛQ^T$.

&emsp;&emsp; Then $A = QΛQ^T$  and use the same $Q$ matrix to define a matrix C, $C = QBQ^T$. 

&emsp;&emsp; According to the trace property, $tr(A) = tr(Λ)$, then $tr(AC) = tr(QΛQ^TQBQ^T) = tr(QABQ^T)$, for $QQ^T = I$.

&emsp;&emsp; Similarity transformation do not change the trace, then $tr(AC) = tr(QABQ^T) = tr(AB)$.

&emsp;&emsp; Since the $A,B$ are the positive semidefinite matrix, then $C$ is also PSD, that means the eigenvalues of $A,C$ are non-negative.
$$tr(AB) = tr(AC) \geq 0$$.



$\bf5.$ Given a symmetric matrix $A ∈ S_n$, let λmin(A) and λmax(A) be the smallest and largest eigenvalues of A. Show that
$$\begin{cases}
λmin(A) ≥μ \\
λmax(A) ≤β      \end{cases}   ⇔  μI⪯A⪯βI
$$
###### &emsp;Proof
&emsp;&emsp; For the left inequality, $ μI⪯A$ is equivilent to show $A - \mu I \geq 0$.
&emsp;&emsp; The eigenvalue of $A - \mu I$ is $eig(A - \mu I) = \{\lambda_1 - \mu,\lambda_2 - \mu,..., \lambda_n - \mu \}$.
&emsp;&emsp; The minimum of the eigenvalue is
$$\mathop{min}\limits_{i}(\lambda - \mu) = \mathop{min}\limits_{i} \lambda - \mu = 0$$
&emsp;&emsp; Therefore, $eig(A - \mu I) \geq 0$, $A-\mu I$ is PSD, that is  $ μI⪯A$.

&emsp;&emsp; For the right inequality, $A⪯βI$ is equivilent to show $\beta I- A \geq 0$.
&emsp;&emsp; The eigenvalue of $\beta I -A $ is $eig(\beta I -A) = \{\beta - \lambda_1 ,\beta - \lambda_2 ,..., \beta - \lambda_n  \}$.
&emsp;&emsp; The minimum of the eigenvalue is
$$\mathop{min}\limits_{i}(\beta-\lambda ) = \beta -\mathop{min}\limits_{i} \lambda = 0$$ 
&emsp;&emsp; Therefore, $eig(\beta I-A) \geq 0$, $\beta I-A$ is PSD, that is  $A⪯βI$.


$\bf6.$ Suppose $fi : R_n → R$, $i = 1,2$ are convex. Show that the pointwise maximum function $f(x) = max{f_1(x),f_2(x)}$ is also convex. 

###### &emsp;Proof
Pointwise maximum function $f(x) =  \mathop{max} {f_1(x),f_2(x)}$
&emsp;&emsp; For convex function $f_i$, for $\alpha \in (0,1)$, pick any $x_1,x_2 \in D$, 
(for some $i\in{1,2}$ )
$$ \begin{aligned} f(\alpha x_1+(1-\alpha)x_2) 
& = f_i(\alpha x_1+(1-\alpha)x_2)  \\
&< \alpha f_i(x_1) + (1-\alpha)f_i(x_2) \\ 
& <\alpha f(x_1) +(1-\alpha) f(x_2) 
\end{aligned}$$
&emsp;&emsp; Therefore, pointwise maximum function  $f(x) =  \mathop{max} {f_1(x),f_2(x)}$ is convex function.  