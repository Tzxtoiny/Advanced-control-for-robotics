## Advanced Control for Robotics - Homework 5

####<p align="right"> 涂志鑫 12131094 </p>
####<p align="right"> 2022.05 </p>  

$\bf1.$  Given a linear system $x' = Ax $ and a quadratic function $V(x) = x^TPx$, where $P$ is an $n×n$ symmetric matrix. Derive the conditions for $P$ under which $V$ will be a Lyapunov function for exponential stability that satisfies $∥x(t)∥^2 ≤ βc^t∥x(0)∥^2$, where $c ∈ (0, 1)$.

###### &emsp;Solution:
&emsp;&emsp; As for the definition of globally exponentially stability, there exit posotive constants $\delta, \lambda, c$
$$|x(t)|\leq C|x(0)|e^{-\lambda t}, \forall |x(0)|\leq \delta $$
&emsp;&emsp; For linear system $\dot{x} = Ax$, its solution is $x(t) = e^{At}x(0)$.

<br>

$\bf2.$ Show that the system  ̇$x = f(x) = \begin{cases}  \dot{x_1} = −x_1 + x_1x_2 \\  \dot{x_2} = −x_2
\end{cases} $ is globally asymptotically stable
(hint: try $V (x) = ln(1 + x_1^2) + x_2^2$ as a Lyapunov function

###### &emsp;Solution
&emsp;&emsp; $f(x) = 0$, for equilibrium, $\left[\begin{matrix} x_1 \\ x_2 \end{matrix}\right ] = \left[\begin{matrix} 0 \\ 0 \end{matrix}\right ]$.

&emsp;&emsp; For $V(x) = ln(1+x_1^2)+x_2^2$, check lyapunov conditions.

&emsp;&emsp; (1) $\forall x=\left[\begin{matrix} x_1 \\ x_2 \end{matrix}\right ]\neq 0$, $V(x) > 0$. For $x=\left[\begin{matrix} x_1 \\ x_2 \end{matrix}\right ] =  0$, $V(x) = 0$, the scalar function $V(x)$ is positive definite (PD).

&emsp;&emsp; (2) $$\begin{aligned} 
Lf {V(x)} &= (\frac{\partial V}{\partial x})^T f(x) = \left [\begin{matrix} \frac{2x_1}{x_1^2+1} & 2x_2 \end{matrix}\right]\left [\begin{matrix}  −x_1 + x_1x_2\\ −x_2 \end{matrix}\right] \\
 &=  \frac{-2x_1^2+2x_1^2x_2}{x_1^2+1} - 2x_2^2 \\
 & =  \frac{-2x_1^2-2x_2^2}{x_1^2+1} 
 \end{aligned}$$
 &emsp;&emsp; $Lf {V(x)} $ is ND.

 &emsp;&emsp; Therefore, we can prove the system is globally asymptotically stable.
<br>

$\bf3.$ Consider a discrete time system $x(k + 1) = Ax(k) + Bu(k)$, with linear feedback law $u(k) = −Kx(k)$. Write down the closed-loop dynamics, and derive conditions for $V (x) = x^T Px$ to be discrete time Lyapunov function for asymptotic closed-loop stability.


###### &emsp;Solution
&emsp;&emsp; The closed-loop discrete time system is $x(k + 1) = (A-BK)x(k)$.

&emsp;&emsp; The initial condition of the system is $x(0)$, when $k = 1$, $x(1) = (A-BK)x(0)$; when $k=2$, $x(2) = (A-BK)^2x(0)$. 

&emsp;&emsp; We can know the dynamics of cloosed-loop system, $x(K) = (A-BK)^kx(0)$.

&emsp;&emsp; For system to be asymptotically stable, the conditions is $(1): V(x)$ is PD. (2) Rate of change of a function $V(x)$ along the system trajectory $\triangle_f V(x) <0$.


&emsp;&emsp; That means for (1): $P$ is PD matrix. For (2): $$\begin{aligned}\triangle_f V(x)  &= V(x(K+1)) - V(x(k)) = V((A-BK)x) - V(x)\\
&=   \end{aligned}$$

$\bf4.$ Show that the PSD cone is acute, i.e., $∀A,B ∈Sn^+$, we have $tr(AB) ≥0$. (Hint: decompose A using unitary matrix Q, i.e. $A = QΛQ^T$ , and then use the same $Q$ to define another matrix $C = QBQ^T$ . The trace $tr(AB)$ can be computed directly in terms of the entries in $C$ and $Λ$)

###### &emsp;Proof
&emsp;&emsp; According to the "Spectral decomposition", for $∀A∈Sn$, there exit a unitary matrix $Q$ and a diagonal matrix $Λ$ satisfy $A = QΛQ^T$.

&emsp;&emsp; Then $A = QΛQ^T$  and use the same $Q$ matrix to define a matrix C, $C = QBQ^T$. 

&emsp;&emsp; According to the trace property, $tr(A) = tr(Λ)$, then $tr(AC) = tr(QΛQ^TQBQ^T) = tr(QABQ^T)$, for $QQ^T = I$.

&emsp;&emsp; Similarity transformation do not change the trace, then $tr(AC) = tr(QABQ^T) = tr(AB)$.

&emsp;&emsp; Since the $A,B$ are the positive semidefinite matrix, then $C$ is also PSD, that means the eigenvalues of $A,C$ are non-negative.
$$tr(AB) = tr(AC) \geq 0$$.



$\bf5.$ Given a symmetric matrix $A ∈ S_n$, let λmin(A) and λmax(A) be the smallest and largest eigenvalues of A. Show that
$$\begin{cases}
λmin(A) ≥μ \\
λmax(A) ≤β      \end{cases}   ⇔  μI⪯A⪯βI
$$
###### &emsp;Proof
&emsp;&emsp; For the left inequality, $ μI⪯A$ is equivilent to show $A - \mu I \geq 0$.
&emsp;&emsp; The eigenvalue of $A - \mu I$ is $eig(A - \mu I) = \{\lambda_1 - \mu,\lambda_2 - \mu,..., \lambda_n - \mu \}$.
&emsp;&emsp; The minimum of the eigenvalue is
$$\mathop{min}\limits_{i}(\lambda - \mu) = \mathop{min}\limits_{i} \lambda - \mu = 0$$
&emsp;&emsp; Therefore, $eig(A - \mu I) \geq 0$, $A-\mu I$ is PSD, that is  $ μI⪯A$.

&emsp;&emsp; For the right inequality, $A⪯βI$ is equivilent to show $\beta I- A \geq 0$.
&emsp;&emsp; The eigenvalue of $\beta I -A $ is $eig(\beta I -A) = \{\beta - \lambda_1 ,\beta - \lambda_2 ,..., \beta - \lambda_n  \}$.
&emsp;&emsp; The minimum of the eigenvalue is
$$\mathop{min}\limits_{i}(\beta-\lambda ) = \beta -\mathop{min}\limits_{i} \lambda = 0$$ 
&emsp;&emsp; Therefore, $eig(\beta I-A) \geq 0$, $\beta I-A$ is PSD, that is  $A⪯βI$.


$\bf6.$ Suppose $fi : R_n → R$, $i = 1,2$ are convex. Show that the pointwise maximum function $f(x) = max{f1(x),f2(x)}$ is also convex. 

###### &emsp;Proof
Pointwise maximum function $f(x) =  \mathop{max} {f1(x),f2(x)}$
&emsp;&emsp; For convex function $f_i$, for $\alpha \in (0,1)$, pick any $x_1,x_2 \in D$, 
(for some $i\in{1,2}$ )
$$ \begin{aligned} f(\alpha x_1+(1-\alpha)x_2) 
& = f_i(\alpha x_1+(1-\alpha)x_2)  \\
&< \alpha f_i(x_1) + (1-\alpha)f_i(x_2) \\ 
& <\alpha f(x_1) +(1-\alpha) f(x_2) 
\end{aligned}$$
&emsp;&emsp; Therefore, pointwise maximum function  $f(x) =  \mathop{max} {f1(x),f2(x)}$ is convex function.  